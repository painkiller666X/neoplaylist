import os
import re
import json
import random
import logging
import requests
from typing import List, Dict, Any, Optional

from repositories.track_repository import get_all_tracks
from playlist.hybrid_tools import extract_json_from_text, log_hybrid_result

# ============================================================
# üéß Motor IA h√≠brido de generaci√≥n de playlists (v2)
# ============================================================

OLLAMA_URL = os.getenv("OLLAMA_URL", "http://127.0.0.1:11434/api/generate")
MODEL_NAME = os.getenv("MODEL_NAME", "neoplaylist-agent")
MAX_RESULTS = int(os.getenv("MAX_RESULTS", "40"))

# Configurar logs
logger = logging.getLogger("playlist.ai_engine")
if not logger.handlers:
    handler = logging.StreamHandler()
    formatter = logging.Formatter("%(asctime)s [%(levelname)s] %(name)s - %(message)s")
    handler.setFormatter(formatter)
    logger.addHandler(handler)
logger.setLevel(logging.INFO)


# ============================================================
# üîπ Utilidades base
# ============================================================

def normalize_text(text: str) -> str:
    """Normaliza texto removiendo s√≠mbolos y pasando a min√∫sculas."""
    return re.sub(r"[^a-zA-Z0-9√°√©√≠√≥√∫√±√º√Å√â√ç√ì√ö√ë√ú ]+", "", text or "").strip().lower()


def build_prompt_from_criteria(criteria: Dict[str, Any]) -> str:
    """Crea un prompt natural a partir de criterios estructurados."""
    prompt = "Genera una playlist musical con "
    parts = []
    if "genre" in criteria:
        parts.append(f"g√©nero {criteria['genre']}")
    if "artist" in criteria:
        parts.append(f"artistas similares a {criteria['artist']}")
    if "mood" in criteria:
        parts.append(f"estado de √°nimo {criteria['mood']}")
    if "year" in criteria:
        parts.append(f"temas de la d√©cada de {criteria['year']}")
    if not parts:
        return "Genera una playlist variada y equilibrada de distintos estilos musicales."
    return prompt + ", ".join(parts) + "."


# ============================================================
# üîπ Llamada robusta a Ollama
# ============================================================

def call_ollama(prompt: str, model: str = MODEL_NAME, temperature: float = 0.7) -> Optional[str]:
    """
    Env√≠a un prompt al modelo Ollama y obtiene la respuesta limpia.
    Maneja fallos de conexi√≥n y timeouts.
    """
    try:
        payload = {
            "model": model,
            "prompt": prompt,
            "stream": False,
            "options": {"temperature": temperature}
        }
        logger.info(f"üß† Enviando prompt a Ollama ({model}): {prompt[:120]}...")
        resp = requests.post(OLLAMA_URL, json=payload, timeout=45)
        resp.raise_for_status()

        data = resp.json()
        text = data.get("response") or data.get("message") or data.get("completion")
        if not text:
            logger.warning("‚ö†Ô∏è Ollama devolvi√≥ respuesta vac√≠a o sin campo 'response'.")
            return None
        return text.strip()

    except requests.Timeout:
        logger.warning("‚è∞ Timeout al llamar a Ollama.")
    except Exception as e:
        logger.error(f"‚ùå Error en llamada a Ollama: {e}")

    return None


# ============================================================
# üîπ Filtro heur√≠stico avanzado
# ============================================================

def heuristic_filter(tracks: List[dict], criteria: Dict[str, Any]) -> List[dict]:
    """
    Aplica filtros heur√≠sticos ponderados (g√©nero, artista, mood, a√±o).
    Retorna los tracks con puntaje y ordenados por relevancia.
    """
    results = []
    for t in tracks:
        score = 0
        if "genre" in criteria and criteria["genre"].lower() in t.get("genre", "").lower():
            score += 3
        if "artist" in criteria and criteria["artist"].lower() in t.get("artist", "").lower():
            score += 4
        if "mood" in criteria and criteria["mood"].lower() in t.get("mood", "").lower():
            score += 2
        if "year" in criteria and str(criteria["year"]) in str(t.get("year", "")):
            score += 1
        if score > 0:
            t["score"] = score
            results.append(t)

    sorted_results = sorted(results, key=lambda x: x.get("score", 0), reverse=True)
    logger.info(f"üéØ {len(sorted_results)} tracks tras filtro heur√≠stico.")
    return sorted_results


# ============================================================
# üîπ IA h√≠brida: sugerir tracks con ayuda de Ollama
# ============================================================

def generate_smart_playlist(criteria: Dict[str, Any]) -> List[dict]:
    """
    Genera una playlist combinando heur√≠stica, razonamiento IA (Ollama) y fallback DB.
    """
    all_tracks = get_all_tracks()
    if not all_tracks:
        logger.warning("‚ö†Ô∏è No hay tracks en la base de datos.")
        return []

    # 1Ô∏è‚É£ Construir prompt
    prompt = criteria.get("prompt") or criteria.get("description") or build_prompt_from_criteria(criteria)
    logger.info(f"üß† Prompt generado: {prompt}")

    # 2Ô∏è‚É£ Llamar a Ollama para sugerencias
    response_text = call_ollama(
        f"{prompt}\nResponde en formato JSON: {{'tracks': ['Artista - Canci√≥n', ...]}}"
    )

    parsed = extract_json_from_text(response_text)
    ai_names = []
    if isinstance(parsed, dict):
        ai_names = parsed.get("tracks") or parsed.get("songs") or []
    elif isinstance(parsed, list):
        ai_names = parsed
    elif isinstance(parsed, str):
        ai_names = [parsed]

    ai_names = [n for n in ai_names if isinstance(n, str) and n.strip()]

    # 3Ô∏è‚É£ Filtro heur√≠stico local
    heuristic_matches = heuristic_filter(all_tracks, criteria)

    # 4Ô∏è‚É£ Vincular sugerencias IA con DB local
    ai_matched = []
    for suggestion in ai_names:
        s_norm = normalize_text(suggestion)
        for t in all_tracks:
            full_name = normalize_text(f"{t.get('artist','')} {t.get('title','')}")
            if s_norm and s_norm in full_name:
                ai_matched.append(t)
                break

    # 5Ô∏è‚É£ Combinar y deduplicar
    combined = {t.get("id") or str(t.get("_id")): t for t in heuristic_matches + ai_matched}.values()
    final_tracks = list(combined)[:MAX_RESULTS]

    # 6Ô∏è‚É£ Fallback si no hay resultados
    if not final_tracks:
        final_tracks = random.sample(all_tracks, min(10, len(all_tracks)))
        logger.info("üé≤ Fallback activado: selecci√≥n aleatoria.")

    # 7Ô∏è‚É£ Registrar resultado h√≠brido
    try:
        log_hybrid_result({
            "criteria": criteria,
            "prompt": prompt,
            "count": len(final_tracks),
            "matches_ai": len(ai_matched),
            "matches_heuristic": len(heuristic_matches),
        })
    except Exception as e:
        logger.debug(f"No se pudo registrar resultado h√≠brido: {e}")

    logger.info(f"‚úÖ Playlist h√≠brida generada con {len(final_tracks)} tracks (IA+Heur√≠stica).")
    return final_tracks

# ============================================================
# üß† Funci√≥n auxiliar: Ejecutar modelo LLM local (Ollama)
# ============================================================
def run_local_llm(prompt: str, model: str = MODEL_NAME, timeout: int = 40) -> str:
    """
    Env√≠a un prompt al modelo local Ollama con manejo robusto de errores.
    Retorna texto limpio o JSON si se detecta estructura v√°lida.
    """
    payload = {"model": model, "prompt": prompt, "stream": False}
    
    try:
        logger.info(f"üß† Enviando prompt al modelo local ({model})")
        res = requests.post(OLLAMA_URL, json=payload, timeout=timeout)
        res.raise_for_status()
        data = res.json()

        raw_text = data.get("response") or data.get("output") or data.get("text") or ""
        
        if raw_text:
            # Limpieza b√°sica (remover delimitadores tipo ```json ... ```)
            cleaned = re.sub(r"^```json\s*", "", raw_text.strip())
            cleaned = re.sub(r"```\s*$", "", cleaned).strip()

            # Intentar parsear JSON
            parsed = extract_json_from_text(cleaned)
            if parsed:
                return parsed

            return cleaned

        logger.warning("‚ö†Ô∏è run_local_llm no devolvi√≥ texto")
        return "{}"
        
    except Exception as e:
        logger.error(f"‚ùå Error en run_local_llm: {e}")
        return "{}"